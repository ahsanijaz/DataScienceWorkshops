---
title       : Linear Classifiers 
subtitle    : 
author      : Ahsan Ijaz
job         : Ebryx
# logo        : ebryx-logo.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
# url:
  # lib: ../../libraries
  # assets: /home/ahsan/Personal/Workshop Machine learning/Workshop1 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
embed : TRUE
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
library(knitr)
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})

knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Classification Examples

> - Given the symptoms, identify the medical condtions of the patient.
> - Online transaction fraud using IP address, past transaction history etc.
> - Detection of DNA mutations as deleterious using data of DNA sequences with and without disease.

---

### Webpage categorization

<img class="center" src= webInput.png  height=450>

---

### Spam detection

<img class="center" src= spam.png  height=450>

---

### Reading mind

<img class="center" src= mindRead.png  height=450>

---

### Personalized healthcare

<img class="center" src= health.png  height=450>

---

## Default of Payment example

- Giving the income and balance, will the person default on payment?
<img class="center" src= default.png  height=450>

---

## Default of Payment example

- Giving the income and balance, will the person default on payment?
<img class="center" src= boxWhisk.png  height=450>

---

## Linear Classification

* Separation of blue and red data points
  * Feature selection task same as in regression
* Two features used in the graph below

```{r, echo=FALSE,fig.align='center',fig.width=4,fig.height=3.5}
library(ggplot2)
library(mvtnorm)

x = seq(-5,20)
line1 = data.frame(var1 = x ,var2 = -3*x + 34, Class = rep( length(x),"1"))

d = as.data.frame(rmvnorm(100, mean = c(3, 3), sigma = matrix(c(5, 0.5, 0.5, 5), nrow = 2)))
d$Class = rep("Class 1",100)

d2 =as.data.frame(rmvnorm(100, mean = c(15, 15), sigma = matrix(c(5, 0.5, 0.5, 5), nrow = 2)))
d2$Class = rep("Class 2",100)

dAll = rbind(d,d2)

m1 = ggplot(NULL) +
    geom_point(data = dAll,aes(x = V1, y=V2, color = Class)) +
    geom_line(data = line1, aes(x=var1, y=var2))

m1

```

---


## Sentiment Analysis Example

<img class="center" src= sentimentAn.png  height=450>

---

## Linear Classifier

Using training data to learn weights for each word.

Words | Coefficients
---------|----------
Good | 0.5
great | 1
Exceptional | 1.5
Awesome | 2.3
bad | -1
terrible| -1.5
hideous | -2.5
awful | -3
we,are,the,where,how,restaurant | 0


---

## Linear Classifier

> - Input sentence 
> - Replace words with coefficients (x)
> - Calculate Score(x) as weighted count of words in sentence
> - If Score(x) > 0, declare positive sentence, else declare negative sentence

---

## Decision Boundaries

For linear classifier:
- When 2 co-efficients are non-zero:
  - A line
- When 3 co-efficients are non-zero:
  - A plane
- When many co-efficients are non-zero:
  - A hyperplane

---

## Simple example of linear classifier


<img class="center" src= classLin.png  height=450>

---


## Simple hyperplane

- Model: $\hat{y}_i = sign(\color{blue}{Score(x_i)})$
- $\color{blue}{Score(x_i)} = w_0 + w_1x_i[1] +  + w_dx_i[d] = w^T\mathbf{x}$
- feature 1 = 1
- feature 2 = #awesome
- feature 3 = #awful
- feature d = #for

---

<img class="center" src= classLinTot.png  height=450>

---

## Probability of each class (confidence of each prediction)

- The food was ok but the service was good. (+1 with prob 0.5)
- The food and service both were amazing. (+1 with prob 0.9)
- The food was awful, so was the service. (-1 with prob 0.9)

---


## Generalized linear models

<img class="center" src= generLM.png  height=450>

---

## Logistic Regression

- sigmoid(Score) = $\frac{1}{1+e^{(-w^T(h(x))}}$ 

<img class="center" src= logit.png  height=400>

---

## Multiclass Training 1 versus all

- $\hat{P}(y=+1|x) = $ estimate of 1 vs all model for each class

- max_prob = 0; $\hat{y} = 0$


- For c = 1,...C:
  - if $\hat{P}_c(y=+1|x)$ > max_prob:
      - $\hat{y}$ = c
      - max_prob = $\hat{P}_c(y=+1|x)$
                      

---

## Logistic regression parameter estimation
<img class="center" src= likelihood.png  height=200>
- Maximize likelihood function over all possible w_0,w_1,W_2 values

$$ l(w_0,w_1,w_2) = P(y_i|x_i,\mathbf{w}) $$

- No closed form solution
- Gradient ascent algortihm to find solution
- While not converged:
$$ w^{t+1} = w^t + \zeta\times{}\frac{dl}{dw}$$

---

## Learning rate
<img class="center" src= learningRate.png  height=200>
- Try several values (exponentially spaced)
  - find one $\zeta$ that is too small
  - another that is too large
- Iterate between the two values.
- Step size that decreases with iterations $\zeta_t = \frac{\zeta_0}{t}$


---

## Loan risk assessment 

<img class="center" src= loanrisk.png  height=400>

---

## Intelligent loan application

<img class="center" src= intelligentLoan.png  height=350>

---

## Decision tree

<img class="center" src= dt.png  height=400>

---

## Quality metric: Classification

- Error measures fraction of mistakes

$$ \color{blue}{\textit{Error} = \frac{\textit{#incorrect predictions}}{\textit{# examples}}} $$

- Best possible value: 0.0
- Worst possible value: 1.0

---

## Greedy decision tree algorithm


---
